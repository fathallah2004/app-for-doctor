{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q3I31HfhktAH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "# Check if running in Colab\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # Local environment (e.g., your PC)\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Google Colab setup\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf==5.29.1 datasets huggingface_hub hf_transfer fsspec==2025.3.2\n",
        "    !pip install --no-deps unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Ey0gtdWll5d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load dataset from uploaded file\n",
        "with open(\"medical_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# Create prompt structure\n",
        "def build_prompt(sample):\n",
        "    return {\n",
        "        \"input\": f\"\"\"### Instruction:\n",
        "You are a medical assistant. Read the medical report and provide:\n",
        "1. A short summary\n",
        "2. Your interpretation\n",
        "3. A possible solution\n",
        "\n",
        "### Input:\n",
        "{sample['report']}\n",
        "\n",
        "### Response:\n",
        "1. Summary: {sample['summary']}\n",
        "2. Interpretation: {sample['interpretation']}\n",
        "3. Solution: {sample['solution']}\"\"\"\n",
        "    }\n",
        "\n",
        "formatted_data = [build_prompt(example) for example in raw_data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oPiX5qzSlvDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a4539c-69df-4bdf-f26a-22bb812e1ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': '### Instruction:\\nYou are a medical assistant. Read the medical report and provide:\\n1. A short summary\\n2. Your interpretation\\n3. A possible solution\\n\\n### Input:\\n45-year-old male presents with 3 days of productive cough, fever (38.5Â°C), and right-sided chest pain. On examination: crackles in right lower lobe. WBC 12.5k, CRP 45. Chest X-ray shows right lower lobe consolidation.\\n\\n### Response:\\n1. Summary: Middle-aged male with community-acquired pneumonia\\n2. Interpretation: Clinical presentation and imaging consistent with bacterial pneumonia, likely Streptococcus pneumoniae\\n3. Solution: 1. Start amoxicillin-clavulanate 875/125mg PO q12h\\n2. Chest physiotherapy\\n3. Follow-up in 48 hours or if symptoms worsen'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0UGbwEBFlyRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa9d69c-0f92-4a8f-c463-0f3a5da7ae22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.8: Fast Mistral patching. Transformers: 4.53.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    max_seq_length = 2048 ,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mxW7dZZml1ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca74e6e3-65e0-437b-f5a4-373dee90ad85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.7.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.0,  # Set to 0 for full Unsloth optimization\n",
        "    bias = \"none\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G2hWGmVXnyXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c1c3ce9f0cd74b4bb58aa047d749d72b",
            "190bb762c2ef4a4e86a4b290c6460259",
            "00c4b7b2bbd34cb2a519ac3b610d892d",
            "d608a3499765443c891334f5d9fad0ac",
            "1ac761e2f74c4a1ea5427abcf2995e99",
            "9ce57b8d801b49cebef0ad0d8a8dc86b",
            "dbcc103eaf9647d2950ba3dbb3a92b83",
            "963dbe246e5d4897a60b23a797118f6c",
            "cba274f6110b49f39aaf58695202bf4d",
            "4ba0373d510e4c9b817844b0a85a0024",
            "d957fa96f71e4139a03e7fc5bce00fcd"
          ]
        },
        "outputId": "e3f16d24-da36-4b63-8293-c96973b5506a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1c3ce9f0cd74b4bb58aa047d749d72b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"input\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CSHOTvLhn4ss"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 2,       # Set to 1 if you get out-of-memory errors\n",
        "    gradient_accumulation_steps = 4,\n",
        "    num_train_epochs = 3,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = True,                           # Mixed precision for speed/memory\n",
        "    logging_steps = 10,\n",
        "    report_to = \"none\",\n",
        "    output_dir = \"outputs\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HYgHzbg9n800"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2SCI9YYbqSKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "274547f3-2166-4377-c481-31776bfd50eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 8 | Num Epochs = 3 | Total steps = 3\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 7,283,675,136 (0.58% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 01:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=6.806591033935547, metrics={'train_runtime': 144.0128, 'train_samples_per_second': 0.167, 'train_steps_per_second': 0.021, 'total_flos': 2109388496044032.0, 'train_loss': 6.806591033935547, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dINufE8Is_V0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ce95b3-4bff-4b13-d2f4-bf3fedb3ca7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('finetuned_med_mistral/tokenizer_config.json',\n",
              " 'finetuned_med_mistral/special_tokens_map.json',\n",
              " 'finetuned_med_mistral/tokenizer.model',\n",
              " 'finetuned_med_mistral/added_tokens.json',\n",
              " 'finetuned_med_mistral/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.save_pretrained(\"finetuned_med_mistral\")\n",
        "tokenizer.save_pretrained(\"finetuned_med_mistral\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9iLpmGSz1g29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e00f22e3-3ca2-4b55-cb9a-9daab58bbdbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "You are a medical assistant. Read the medical report and provide:\n",
            "[SUMMARY]\n",
            "[INTERPRETATION]\n",
            "[SOLUTION]\n",
            "\n",
            "### Input:\n",
            "A 65-year-old male with fever and cough.\n",
            "\n",
            "### Response:\n",
            "### Instruction:\n",
            "You are a medical assistant. Read the medical report and provide:\n",
            "[SUMMARY]\n",
            "[INTERPRETATION]\n",
            "[SOLUTION]\n",
            "\n",
            "### Input:\n",
            "A 65-year-old male with fever and cough.\n",
            "\n",
            "### Response:\n",
            "The patient is a 65-year-old male with a history of hypertension and diabetes. He presents with a 2-week history of fever, cough, and shortness of breath. On physical examination, he is afebrile, tachycardic, and tachypneic. Chest X-ray shows bilateral infiltrates. Laboratory tests show a white blood cell count of 12,000/mm3, hemoglobin of 10 g/dL, and platelets of 100,000/mm3. The patient is diagnosed with pneumonia and started on antibiotics.\n",
            "\n",
            "### Summary:\n",
            "The patient is a 65-year-old male with a history of hypertension and diabetes. He presents with a 2-week history of fever, cough, and shortness of breath. On physical examination, he is afebrile, tachycardic, and tachypneic. Chest X-ray shows bilateral infiltrates. Laboratory tests show a white blood cell count of 12,000/mm3, hemoglobin of 10 g/dL, and platelets of 100,000/mm3. The patient is diagnosed with pneumonia and started on antibiotics.\n",
            "\n",
            "### Interpretation:\n",
            "The patient is a 65-year-old male with a history of hypertension and diabetes. He presents with a 2-week history of fever, cough, and shortness of breath. On physical examination, he is afebrile, tachycardic, and tachypneic. Chest X-ray shows bilateral infiltrates. Laboratory tests show a white blood cell count of 12,000/mm3, hemoglobin of 10 g/dL, and platelets of 100,000/mm3. The patient is diagnosed with pneumonia and started on antibiotics.\n",
            "\n",
            "### Solution:\n",
            "The patient is a 65-year-old male with a history of hypertension and diabetes. He presents with a 2-week history of fever, cough, and shortness of breath. On physical examination, he\n"
          ]
        }
      ],
      "source": [
        "sample_report = \"A 65-year-old male with fever and cough.\"\n",
        "prompt = f\"\"\"### Instruction:\n",
        "You are a medical assistant. Read the medical report and provide:\n",
        "[SUMMARY]\n",
        "[INTERPRETATION]\n",
        "[SOLUTION]\n",
        "\n",
        "### Input:\n",
        "{sample_report}\n",
        "\n",
        "### Response:\"\"\"\n",
        "print(prompt)\n",
        "\n",
        "\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=500)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl9RoPzmlHBm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "17hu6LnP1mTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f319a49b-8eb4-4d39-e6b0-77320edf2e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "--2025-07-26 16:02:48--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.7.0/cloudflared-linux-amd64 [following]\n",
            "--2025-07-26 16:02:48--  https://github.com/cloudflare/cloudflared/releases/download/2025.7.0/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/106867604/37d2bad8-a2ed-4b93-8139-cbb15162d81d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-26T16%3A51%3A25Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-26T15%3A50%3A57Z&ske=2025-07-26T16%3A51%3A25Z&sks=b&skv=2018-11-09&sig=ODEgemOY%2BUkNY6OQQ4Q58877opUaYHqwMSd6513REqs%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzU0NjA2OCwibmJmIjoxNzUzNTQ1NzY4LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.UF1LpzhfmheIrzI3tQFwdp6hUio7jJe_Y6GKskvo2zM&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-26 16:02:48--  https://release-assets.githubusercontent.com/github-production-release-asset/106867604/37d2bad8-a2ed-4b93-8139-cbb15162d81d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-26T16%3A51%3A25Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-26T15%3A50%3A57Z&ske=2025-07-26T16%3A51%3A25Z&sks=b&skv=2018-11-09&sig=ODEgemOY%2BUkNY6OQQ4Q58877opUaYHqwMSd6513REqs%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzU0NjA2OCwibmJmIjoxNzUzNTQ1NzY4LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.UF1LpzhfmheIrzI3tQFwdp6hUio7jJe_Y6GKskvo2zM&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41168761 (39M) [application/octet-stream]\n",
            "Saving to: â€˜cloudflaredâ€™\n",
            "\n",
            "cloudflared         100%[===================>]  39.26M   252MB/s    in 0.2s    \n",
            "\n",
            "2025-07-26 16:02:48 (252 MB/s) - â€˜cloudflaredâ€™ saved [41168761/41168761]\n",
            "\n",
            "Requirement already satisfied: fastapi-cache2 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from fastapi-cache2) (0.116.1)\n",
            "Requirement already satisfied: pendulum<4.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-cache2) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-cache2) (4.14.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from fastapi-cache2) (0.35.0)\n",
            "Requirement already satisfied: anyio>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from sse-starlette) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=4.7.0->sse-starlette) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=4.7.0->sse-starlette) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6 in /usr/local/lib/python3.11/dist-packages (from pendulum<4.0.0,>=3.0.0->fastapi-cache2) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pendulum<4.0.0,>=3.0.0->fastapi-cache2) (2025.2)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->fastapi-cache2) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi->fastapi-cache2) (2.11.7)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->fastapi-cache2) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->fastapi-cache2) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->fastapi-cache2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->fastapi-cache2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->fastapi-cache2) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.6->pendulum<4.0.0,>=3.0.0->fastapi-cache2) (1.17.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn nest-asyncio\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "!pip install fastapi-cache2 sse-starlette python-multipart\n",
        "!pip install gradio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ia-KgK06Hwxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef19d34-8f27-409b-fe66-f86afef718fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.7.8: Fast Mistral patching. Transformers: 4.53.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Configure 4-bit quantization with CPU offload fallback\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    llm_int8_enable_fp32_cpu_offload=True  # âœ… allows CPU fallback\n",
        ")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"finetuned_med_mistral\",\n",
        "    max_seq_length=4096,\n",
        "    dtype=None,  # Let Unsloth auto-select best dtype\n",
        "    device_map=\"auto\",  # âœ… Smart device placement\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1CBYfD8THzb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80251cb9-995b-4f6b-a906-f07f807b46a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.7.8: Fast Mistral patching. Transformers: 4.53.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.responses import StreamingResponse\n",
        "from pydantic import BaseModel\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "import re\n",
        "from typing import List\n",
        "import logging\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Enable CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Model config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        ")\n",
        "\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"finetuned_med_mistral\",\n",
        "        max_seq_length=3072,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    model.eval()\n",
        "    logger.info(\"âœ… Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"âŒ Model loading failed: {str(e)}\")\n",
        "    raise RuntimeError(\"Model load error\")\n",
        "\n",
        "class MedicalReportRequest(BaseModel):\n",
        "    report: str\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    # Remove repeated phrases, excessive whitespace, unwanted chars, unicode normalize\n",
        "    text = re.sub(r'(?s)(\\b\\w+\\b.*?)(?:\\s*\\1)+', r'\\1', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'[^\\w\\s.,:;!?()\\-/]', ' ', text)  # allow standard punctuation\n",
        "    text = re.sub(r'\\\\\\(.*?\\\\\\)', '', text)  # remove latex math if present\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)  # remove bracketed citations\n",
        "    text = re.sub(r'\\[\\^.*?\\]', '', text)\n",
        "    text = re.sub(r'\\bdoi:\\S+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\bpmid:\\S+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text: str, max_words: int = 200) -> List[str]:\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    word_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        if word_count + len(words) <= max_words:\n",
        "            current_chunk.append(sentence)\n",
        "            word_count += len(words)\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "            word_count = len(words)\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "def post_process_output(output: str) -> str:\n",
        "    # Remove everything between [INST]...[/INST]\n",
        "    output = re.sub(r'\\[INST\\].*?\\[/INST\\]', '', output, flags=re.DOTALL)\n",
        "\n",
        "    # Remove boilerplate like \"Chapter 1 Introduction\" or numeric headers\n",
        "    output = re.sub(r'## Chapter \\d+.*', '', output, flags=re.IGNORECASE)\n",
        "    output = re.sub(r'## \\d+(\\.\\d+)+\\.*', '', output)\n",
        "\n",
        "    # Remove repeated headers, bracketed phrases, URLs, and noise\n",
        "    output = re.sub(r'(## [^\\n]+)(\\s+\\1)+', r'\\1', output)\n",
        "    output = re.sub(r'\\[[^\\]]+?\\]', '', output)\n",
        "    output = re.sub(r'https?://\\S+\\s?', '', output)\n",
        "\n",
        "    # Remove residual prompt header lines and role/task lines\n",
        "    output = re.sub(r'(ROLE|TASK|RULES|CLINICAL REPORT|MEDICAL REPORT):.*', '', output, flags=re.IGNORECASE)\n",
        "\n",
        "    # Deduplicate repeated lines\n",
        "    lines = output.split('\\n')\n",
        "    filtered_lines = []\n",
        "    seen = set()\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        if not stripped or stripped in seen:\n",
        "            continue\n",
        "        # Filter obvious boilerplate/noise lines\n",
        "        if re.search(r'\\bfractional state\\b|\\bmath\\b|\\bchapter\\b|\\breference\\b', stripped, re.IGNORECASE):\n",
        "            continue\n",
        "        if re.search(r'[@#$%^&*{}<>~`\\[\\]]', stripped):\n",
        "            continue\n",
        "        filtered_lines.append(line)\n",
        "        seen.add(stripped)\n",
        "\n",
        "    cleaned = '\\n'.join(filtered_lines).strip()\n",
        "    cleaned = re.sub(r'\\n{2,}', '\\n\\n', cleaned)  # collapse multiple empty lines\n",
        "    return cleaned\n",
        "\n",
        "def generate_with_fallback(\n",
        "    prompt: str,\n",
        "    max_tokens: int,\n",
        "    temp: float = 0.1,\n",
        "    do_sample: bool = False,\n",
        "    repetition_penalty: float = 1.1,\n",
        "    no_repeat_ngram_size: int = 3\n",
        ") -> str:\n",
        "    try:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1536).to(\"cuda\")\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=min(max_tokens, 1200),\n",
        "            temperature=temp,\n",
        "            do_sample=do_sample,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if result.startswith(prompt[:100]):\n",
        "            result = result[len(prompt):].strip()\n",
        "        return result.strip().split(\"###\")[0]\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Generation failed: {e}\")\n",
        "\n",
        "# Refined stricter prompts forbidding narrative and educational content, only facts from report.\n",
        "\n",
        "\n",
        "def generate_structured_summary(report: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "[INST] <<SYS>>\n",
        "ROLE: Clinical Data Extractor\n",
        "\n",
        "Extract ONLY data explicitly written in the report. NO assumptions.\n",
        "No narrative or educational explanations. Use exact quotes for facts.\n",
        "\n",
        "Extract ONLY:\n",
        "1. Demographics (Age/Sex/Risks)\n",
        "2. Chief Complaint + Duration\n",
        "3. Key Abnormalities (ECG/Labs/Imaging/Vitals)\n",
        "4. Interventions Done\n",
        "5. Outcome / Patient status\n",
        "\n",
        "Rules:\n",
        "- Use exact quoted phrases.\n",
        "- Missing info â†’ [Not documented].\n",
        "- Do NOT add information not in the report.\n",
        "- Do NOT include any external facts or guidelines.\n",
        "- Output ONLY the extracted data.\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "\\\"\\\"\\\"{report[:1800]}\\\"\\\"\\\"\n",
        "[/INST]\n",
        "SUMMARY:\n",
        "\"\"\"\n",
        "    return generate_with_fallback(prompt, max_tokens=600, temp=0.1)\n",
        "\n",
        "\n",
        "def generate_focused_interpretation(report: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "[INST] <<SYS>>\n",
        "ROLE: Lead Diagnostician\n",
        "\n",
        "Goal: Provide the most likely diagnosis supported STRICTLY by facts in the report.\n",
        "\n",
        "Include:\n",
        "- DX: [Condition]\n",
        "- 2 clear quotes supporting diagnosis\n",
        "- 1 unclear/conflicting data if present\n",
        "\n",
        "NO assumptions, NO narratives, NO external info.\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "\\\"\\\"\\\"{report[:1800]}\\\"\\\"\\\"\n",
        "[/INST]\n",
        "INTERPRETATION:\n",
        "\"\"\"\n",
        "    return generate_with_fallback(prompt, max_tokens=800, temp=0.1)\n",
        "\n",
        "\n",
        "def generate_case_specific_solutions(report: str, diagnosis: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "[INST] <<SYS>>\n",
        "ROLE: Tactical Medical Planner\n",
        "\n",
        "Given DX: {diagnosis}\n",
        "\n",
        "Provide ACTUAL recommendations STRICTLY based on the report facts, no assumptions:\n",
        "\n",
        "1. IMMEDIATE: Life-threatening actions + Quote + Metrics + Backup plan\n",
        "2. URGENT: Time-sensitive actions + Quote + Metrics + Backup plan\n",
        "3. ROUTINE: Follow-up actions + Quote + Metrics + Backup plan\n",
        "\n",
        "Do NOT add educational or hypothetical information.\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "\\\"\\\"\\\"{report[:1800]}\\\"\\\"\\\"\n",
        "[/INST]\n",
        "SOLUTIONS:\n",
        "\"\"\"\n",
        "    return generate_with_fallback(prompt, max_tokens=1200, temp=0.1)\n",
        "\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "def analyze_report(request: MedicalReportRequest):\n",
        "    MAX_REPORT_LENGTH = 8000\n",
        "\n",
        "    if not request.report.strip():\n",
        "        raise HTTPException(status_code=400, detail=\"Empty report\")\n",
        "    if len(request.report) > MAX_REPORT_LENGTH:\n",
        "        raise HTTPException(status_code=400, detail=\"Report too long\")\n",
        "\n",
        "    def generate_output():\n",
        "        try:\n",
        "            clean_report = clean_text(request.report)\n",
        "            chunks = chunk_text(clean_report)\n",
        "            yield f\"ðŸ”Ž Processing {len(chunks)} chunks...\\n\\n\"\n",
        "\n",
        "            # Generate structured summary\n",
        "            summary_raw = generate_structured_summary(clean_report)\n",
        "            summary = post_process_output(summary_raw)\n",
        "            yield f\"ðŸ“‹ Structured Summary:\\n{summary}\\n\\n\"\n",
        "\n",
        "            # Generate focused clinical interpretation\n",
        "            interpretation_raw = generate_focused_interpretation(clean_report)\n",
        "            interpretation = post_process_output(interpretation_raw)\n",
        "            yield f\"ðŸ©º Clinical Interpretation:\\n{interpretation}\\n\\n\"\n",
        "\n",
        "            # Parse primary diagnosis to pass to solutions\n",
        "            primary_dx = \"unspecified condition\"\n",
        "            if 'DX:' in interpretation:\n",
        "                primary_dx_line = interpretation.split('DX:')[-1].split('\\n')[0].strip()\n",
        "                if primary_dx_line:\n",
        "                    primary_dx = primary_dx_line\n",
        "\n",
        "            # Generate detailed recommendations\n",
        "            solutions_raw = generate_case_specific_solutions(clean_report, primary_dx)\n",
        "            solutions = post_process_output(solutions_raw)\n",
        "            yield f\"ðŸ§­ Recommendations:\\n{solutions}\\n\"\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            yield \"âŒ GPU out of memory. Use a shorter report.\\n\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis error: {str(e)}\", exc_info=True)\n",
        "            yield f\"âŒ Error: {str(e)}\\n\"\n",
        "\n",
        "    return StreamingResponse(generate_output(), media_type=\"text/plain\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xz1TxXlXH1jy"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "import threading\n",
        "import uvicorn\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "threading.Thread(target=run).start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-JOx8HkNfRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf51940-89dc-4d00-e3a9-24794593b40b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [9581]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-07-26T16:03:41Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-07-26T16:03:41Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m |  https://address-beginners-theory-dynamic.trycloudflare.com                                |\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.7.0 (Checksum 51e3909335fd7ba2ed5c696b0a6fb7d4a74f6a15bf36615cea0fccba620cfb3f)\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 metrics:127.0.0.1:45678 no-autoupdate:true protocol:quic url:http://localhost:8000]\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 9ce4d55a-61d2-480a-988e-411e49ae7892\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:45678/metrics\n",
            "\u001b[90m2025-07-26T16:03:46Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.47\n",
            "2025/07/26 16:03:46 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-07-26T16:03:47Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m3305fe00-6974-42af-9bd1-573145abcef3 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.47 \u001b[36mlocation=\u001b[0msin17 \u001b[36mprotocol=\u001b[0mquic\n",
            "INFO:     2c0f:f698:c236:f0a:8921:bfad:5a4f:ac:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2c0f:f698:c236:f0a:8921:bfad:5a4f:ac:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2c0f:f698:c236:f0a:8921:bfad:5a4f:ac:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2c0f:f698:c236:f0a:8921:bfad:5a4f:ac:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     2c0f:f698:c236:f0a:8921:bfad:5a4f:ac:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2c0f:f698:c236:f0a:8921:bfad:5a4f:ac:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2c0f:f698:c236:f0a:8921:bfad:5a4f:ac:0 - \"OPTIONS /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2c0f:f698:c236:f0a:8921:bfad:5a4f:ac:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     2c0f:f698:c236:f0a:8921:bfad:5a4f:ac:0 - \"POST /analyze HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "!./cloudflared tunnel --url http://localhost:8000 --metrics 127.0.0.1:45678 --no-autoupdate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import requests\n",
        "\n",
        "url = \"https://useful-candle-9d.trycloudflare.com/analyze\"  # Replace with your actual URL\n",
        "\n",
        "response = requests.post(url, json={\n",
        "    \"report\": \"A 70-year-old patient reports difficulty breathing and swelling in the ankles...\"\n",
        "})\n",
        "\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "tKeiIgfRoqBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1c3ce9f0cd74b4bb58aa047d749d72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_190bb762c2ef4a4e86a4b290c6460259",
              "IPY_MODEL_00c4b7b2bbd34cb2a519ac3b610d892d",
              "IPY_MODEL_d608a3499765443c891334f5d9fad0ac"
            ],
            "layout": "IPY_MODEL_1ac761e2f74c4a1ea5427abcf2995e99"
          }
        },
        "190bb762c2ef4a4e86a4b290c6460259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ce57b8d801b49cebef0ad0d8a8dc86b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dbcc103eaf9647d2950ba3dbb3a92b83",
            "value": "Map:â€‡100%"
          }
        },
        "00c4b7b2bbd34cb2a519ac3b610d892d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_963dbe246e5d4897a60b23a797118f6c",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cba274f6110b49f39aaf58695202bf4d",
            "value": 8
          }
        },
        "d608a3499765443c891334f5d9fad0ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ba0373d510e4c9b817844b0a85a0024",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d957fa96f71e4139a03e7fc5bce00fcd",
            "value": "â€‡8/8â€‡[00:00&lt;00:00,â€‡111.63â€‡examples/s]"
          }
        },
        "1ac761e2f74c4a1ea5427abcf2995e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce57b8d801b49cebef0ad0d8a8dc86b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbcc103eaf9647d2950ba3dbb3a92b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "963dbe246e5d4897a60b23a797118f6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cba274f6110b49f39aaf58695202bf4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ba0373d510e4c9b817844b0a85a0024": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d957fa96f71e4139a03e7fc5bce00fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}