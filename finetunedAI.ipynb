{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q3I31HfhktAH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "# Check if running in Colab\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # Local environment (e.g., your PC)\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Google Colab setup\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf==5.29.1 datasets huggingface_hub hf_transfer fsspec==2025.3.2\n",
        "    !pip install --no-deps unsloth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Ey0gtdWll5d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load dataset from uploaded file\n",
        "with open(\"medical_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# Create prompt structure\n",
        "def build_prompt(sample):\n",
        "    return {\n",
        "        \"input\": f\"\"\"### Instruction:\n",
        "You are a medical assistant. Read the medical report and provide:\n",
        "1. A short summary\n",
        "2. Your interpretation\n",
        "3. A possible solution\n",
        "\n",
        "### Input:\n",
        "{sample['report']}\n",
        "\n",
        "### Response:\n",
        "1. Summary: {sample['summary']}\n",
        "2. Interpretation: {sample['interpretation']}\n",
        "3. Solution: {sample['solution']}\"\"\"\n",
        "    }\n",
        "\n",
        "formatted_data = [build_prompt(example) for example in raw_data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPiX5qzSlvDJ",
        "outputId": "2c172b94-6fc2-4e8e-dd0f-3a8cc7eed24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': '### Instruction:\\nYou are a medical assistant. Read the medical report and provide:\\n1. A short summary\\n2. Your interpretation\\n3. A possible solution\\n\\n### Input:\\n45-year-old male presents with 3 days of productive cough, fever (38.5Â°C), and right-sided chest pain. On examination: crackles in right lower lobe. WBC 12.5k, CRP 45. Chest X-ray shows right lower lobe consolidation.\\n\\n### Response:\\n1. Summary: Middle-aged male with community-acquired pneumonia\\n2. Interpretation: Clinical presentation and imaging consistent with bacterial pneumonia, likely Streptococcus pneumoniae\\n3. Solution: 1. Start amoxicillin-clavulanate 875/125mg PO q12h\\n2. Chest physiotherapy\\n3. Follow-up in 48 hours or if symptoms worsen'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0UGbwEBFlyRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa42ac0-682c-480b-d597-bca9d800e8b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.3: Fast Mistral patching. Transformers: 4.53.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    max_seq_length = 2048 ,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mxW7dZZml1ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7195b0f-97be-4858-e442-82c0340e0e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.7.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.0,  # Set to 0 for full Unsloth optimization\n",
        "    bias = \"none\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G2hWGmVXnyXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f0c3c84d6ee54443a6425d6fe4d03ef6",
            "19993a32685c42719a4a4c2150f45ef3",
            "6a05c86e91be44f890bfd1014be9a226",
            "e2c2240984eb4186a8b05f1a820e6810",
            "d8699d4ba45e4019b8524009c52db26e",
            "5454e0eb0edc454f9dc2275c9bad2898",
            "870751378eef4858aa8be004243194f9",
            "194b8c6f5e2c44fd955a080aec623e05",
            "54614ebbc1424ff5b5251e09e4677b61",
            "16de58dfc3fb4d8995b9503f11883bdf",
            "cc7ade48e6af4e75956c427a68564694"
          ]
        },
        "outputId": "d35560f7-2067-406e-927d-da1f7da69159"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0c3c84d6ee54443a6425d6fe4d03ef6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"input\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CSHOTvLhn4ss"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 2,       # Set to 1 if you get out-of-memory errors\n",
        "    gradient_accumulation_steps = 4,\n",
        "    num_train_epochs = 3,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = True,                           # Mixed precision for speed/memory\n",
        "    logging_steps = 10,\n",
        "    report_to = \"none\",\n",
        "    output_dir = \"outputs\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HYgHzbg9n800"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2SCI9YYbqSKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "4fdf9e29-674a-4de8-bab1-2f202e8d4108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 8 | Num Epochs = 3 | Total steps = 3\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 7,283,675,136 (0.58% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 01:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=6.806591033935547, metrics={'train_runtime': 153.6307, 'train_samples_per_second': 0.156, 'train_steps_per_second': 0.02, 'total_flos': 2109388496044032.0, 'train_loss': 6.806591033935547, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dINufE8Is_V0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f92792-bc47-41b7-d20c-8234289c468a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('finetuned_med_mistral/tokenizer_config.json',\n",
              " 'finetuned_med_mistral/special_tokens_map.json',\n",
              " 'finetuned_med_mistral/tokenizer.model',\n",
              " 'finetuned_med_mistral/added_tokens.json',\n",
              " 'finetuned_med_mistral/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.save_pretrained(\"finetuned_med_mistral\")\n",
        "tokenizer.save_pretrained(\"finetuned_med_mistral\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9iLpmGSz1g29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3731171f-445d-4bda-f132-63a3d4c2d4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "You are a medical assistant. Read the medical report and provide:\n",
            "[SUMMARY]\n",
            "[INTERPRETATION]\n",
            "[SOLUTION]\n",
            "\n",
            "### Input:\n",
            "A 65-year-old male with fever and cough.\n",
            "\n",
            "### Response:\n",
            "### Instruction:\n",
            "You are a medical assistant. Read the medical report and provide:\n",
            "[SUMMARY]\n",
            "[INTERPRETATION]\n",
            "[SOLUTION]\n",
            "\n",
            "### Input:\n",
            "A 65-year-old male with fever and cough.\n",
            "\n",
            "### Response:\n",
            "The patient is a 65-year-old male with a history of hypertension and diabetes. He presents with a 2-week history of fever, cough, and shortness of breath. On physical examination, he is afebrile, tachycardic, and tachypneic. Chest X-ray shows bilateral infiltrates. Laboratory tests show a white blood cell count of 12,000/mm3, hemoglobin of 10 g/dL, and platelets of 100,000/mm3. The patient is diagnosed with pneumonia and started on antibiotics.\n",
            "\n",
            "### Summary:\n",
            "The patient is a 65-year-old male with a history of hypertension and diabetes. He presents with a 2-week history of fever, cough, and shortness of breath. On physical examination, he is afebrile, tachycardic, and tachypneic. Chest X-ray shows bilateral infiltrates. Laboratory tests show a white blood cell count of 12,000/mm3, hemoglobin of 10 g/dL, and platelets of 100,000/mm3. The patient is diagnosed with pneumonia and started on antibiotics.\n",
            "\n",
            "### Interpretation:\n",
            "The patient is a 65-year-old male with a history of hypertension and diabetes. He presents with a 2-week history of fever, cough, and shortness of breath. On physical examination, he is afebrile, tachycardic, and tachypneic. Chest X-ray shows bilateral infiltrates. Laboratory tests show a white blood cell count of 12,000/mm3, hemoglobin of 10 g/dL, and platelets of 100,000/mm3. The patient is diagnosed with pneumonia and started on antibiotics.\n",
            "\n",
            "### Solution:\n",
            "The patient is a 65-year-old male with a history of hypertension and diabetes. He presents with a 2-week history of fever, cough, and shortness of breath. On physical examination, he\n"
          ]
        }
      ],
      "source": [
        "sample_report = \"A 65-year-old male with fever and cough.\"\n",
        "prompt = f\"\"\"### Instruction:\n",
        "You are a medical assistant. Read the medical report and provide:\n",
        "[SUMMARY]\n",
        "[INTERPRETATION]\n",
        "[SOLUTION]\n",
        "\n",
        "### Input:\n",
        "{sample_report}\n",
        "\n",
        "### Response:\"\"\"\n",
        "print(prompt)\n",
        "\n",
        "\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=500)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl9RoPzmlHBm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "17hu6LnP1mTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91105917-27f4-417b-86c2-529d5127c5fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "--2025-07-11 19:42:08--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.7.0/cloudflared-linux-amd64 [following]\n",
            "--2025-07-11 19:42:09--  https://github.com/cloudflare/cloudflared/releases/download/2025.7.0/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/106867604/37d2bad8-a2ed-4b93-8139-cbb15162d81d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-11T20%3A41%3A40Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-11T19%3A41%3A40Z&ske=2025-07-11T20%3A41%3A40Z&sks=b&skv=2018-11-09&sig=oxbvtZ9sl6dndLB0CIYpZEAkx9tdRlnft0LO0SS9xfE%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MjI2MzIyOSwibmJmIjoxNzUyMjYyOTI5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.AKv_TOR1aAwjFT_UnhjQEVnjVJPO4IkdQhTBUCRQFGg&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-11 19:42:09--  https://release-assets.githubusercontent.com/github-production-release-asset/106867604/37d2bad8-a2ed-4b93-8139-cbb15162d81d?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-11T20%3A41%3A40Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-11T19%3A41%3A40Z&ske=2025-07-11T20%3A41%3A40Z&sks=b&skv=2018-11-09&sig=oxbvtZ9sl6dndLB0CIYpZEAkx9tdRlnft0LO0SS9xfE%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MjI2MzIyOSwibmJmIjoxNzUyMjYyOTI5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.AKv_TOR1aAwjFT_UnhjQEVnjVJPO4IkdQhTBUCRQFGg&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41168761 (39M) [application/octet-stream]\n",
            "Saving to: â€˜cloudflaredâ€™\n",
            "\n",
            "cloudflared         100%[===================>]  39.26M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-07-11 19:42:09 (368 MB/s) - â€˜cloudflaredâ€™ saved [41168761/41168761]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn nest-asyncio\n",
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ia-KgK06Hwxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa59f1e-cd9e-4319-90ce-4dfa1ba76292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.7.3: Fast Mistral patching. Transformers: 4.53.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Configure 4-bit quantization with CPU offload fallback\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    llm_int8_enable_fp32_cpu_offload=True  # âœ… allows CPU fallback\n",
        ")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"finetuned_med_mistral\",\n",
        "    max_seq_length=4096,\n",
        "    dtype=None,  # Let Unsloth auto-select best dtype\n",
        "    device_map=\"auto\",  # âœ… Smart device placement\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1CBYfD8THzb6"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "import re\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Or replace \"*\" with the actual domain for security\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "class MedicalReportRequest(BaseModel):\n",
        "    report: str\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "def analyze_report(request: MedicalReportRequest):\n",
        "    # Prompt template to guide the model\n",
        "    prompt_template = \"\"\"### Instruction:\n",
        "You are a medical assistant. Read the following medical report and provide:\n",
        "\n",
        "1. A short summary\n",
        "2. Your interpretation\n",
        "3. A solution â€” structured as multiple clearly numbered steps (e.g., 1., 2., 3.) on separate lines\n",
        "\n",
        "### Report:\n",
        "{report}\n",
        "\n",
        "### Response:\n",
        "1. Summary:\"\"\"\n",
        "\n",
        "    prompt = prompt_template.format(report=request.report)\n",
        "\n",
        "    # Tokenize and send to GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"=== RAW DECODED OUTPUT ===\")\n",
        "    print(decoded)\n",
        "\n",
        "    # Regex parsing\n",
        "    summary = re.search(r\"1\\. Summary:\\s*(.*?)(?:2\\. Interpretation:|3\\.|$)\", decoded, re.DOTALL)\n",
        "    interpretation = re.search(r\"2\\. Interpretation:\\s*(.*?)(?:3\\.(?: Solution| Possible Solution):|$)\", decoded, re.DOTALL)\n",
        "    solution = re.search(r\"3\\.(?: Solution| Possible Solution):\\s*(.*)\", decoded, re.DOTALL)\n",
        "\n",
        "    # Deduplicate lines\n",
        "    def deduplicate_solution(text):\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        seen = set()\n",
        "        cleaned = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line and line not in seen:\n",
        "                seen.add(line)\n",
        "                cleaned.append(line)\n",
        "        return \"\\n\".join(cleaned)\n",
        "\n",
        "    # Auto-number if only one block or unnumbered\n",
        "    def auto_number_if_needed(text):\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        if len(lines) == 1:\n",
        "            return \"1. \" + lines[0]\n",
        "        if any(line.strip().startswith(\"1.\") for line in lines):\n",
        "            return text\n",
        "        return \"\\n\".join(f\"{i+1}. {line.strip()}\" for i, line in enumerate(lines) if line.strip())\n",
        "\n",
        "    # Split compound sentences into separate steps using regex instead of nltk\n",
        "    def split_sentences_into_numbered_steps(text):\n",
        "        if sum(1 for line in text.splitlines() if line.strip().startswith(tuple(f\"{i}.\" for i in range(1, 10)))) > 1:\n",
        "            return text\n",
        "        sentences = re.split(r'(?<=[.!?;])\\s+', text.strip())\n",
        "        return \"\\n\".join(f\"{i+1}. {s.strip()}\" for i, s in enumerate(sentences) if s.strip())\n",
        "\n",
        "    # Apply all processing steps\n",
        "    cleaned_solution = deduplicate_solution(solution.group(1).strip()) if solution else \"\"\n",
        "    auto_numbered = auto_number_if_needed(cleaned_solution)\n",
        "    final_solution = split_sentences_into_numbered_steps(auto_numbered)\n",
        "\n",
        "    return {\n",
        "        \"summary\": summary.group(1).strip() if summary else \"\",\n",
        "        \"interpretation\": interpretation.group(1).strip() if interpretation else \"\",\n",
        "        \"solution\": final_solution\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xz1TxXlXH1jy"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "import threading\n",
        "import uvicorn\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "threading.Thread(target=run).start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-JOx8HkNfRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c164ab-b6fa-4ef5-a32e-1e4f7f50b1ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [33566]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-07-11T19:42:31Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-07-11T19:42:31Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m |  https://ids-cruise-detection-fisher.trycloudflare.com                                     |\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.7.0 (Checksum 51e3909335fd7ba2ed5c696b0a6fb7d4a74f6a15bf36615cea0fccba620cfb3f)\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8000]\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 02211dbe-61fe-46a0-9155-0a3649391694\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-07-11T19:42:36Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.57\n",
            "2025/07/11 19:42:36 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-07-11T19:42:37Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0md225532c-258f-4a5d-a9d8-9d7cb42240d1 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.57 \u001b[36mlocation=\u001b[0msin09 \u001b[36mprotocol=\u001b[0mquic\n",
            "INFO:     197.26.153.79:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     197.26.153.79:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAW DECODED OUTPUT ===\n",
            "### Instruction:\n",
            "You are a medical assistant. Read the following medical report and provide:\n",
            "\n",
            "1. A short summary\n",
            "2. Your interpretation\n",
            "3. A solution â€” structured as multiple clearly numbered steps (e.g., 1., 2., 3.) on separate lines\n",
            "\n",
            "### Report:\n",
            "A 30-year-old female presents with high-grade fever, severe headache, photophobia, and neck stiffness. She is lethargic and has a positive Brudzinski sign.\n",
            "\n",
            "### Response:\n",
            "1. Summary: The patient has meningitis.\n",
            "2. Interpretation: Meningitis is an inflammation of the membranes that cover the brain and spinal cord. It can be caused by bacteria or viruses. Symptoms include fever, headache, nausea, vomiting, confusion, seizures, and stiff neck. Treatment includes antibiotics for bacterial infections and antiviral medications for viral infections.\n",
            "3. Solution: Administer IV fluids to maintain hydration; administer pain medication such as acetaminophen or ibuprofen; monitor vital signs closely; consult with neurologist if symptoms worsen or do not improve within 48 hours after starting treatment.\n",
            "INFO:     197.26.153.79:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     197.26.153.79:0 - \"OPTIONS /analyze HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     197.26.153.79:0 - \"OPTIONS /analyze HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     197.26.153.79:0 - \"POST /analyze HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 112, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 93, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 144, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 714, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 734, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 214, in run_endpoint_function\n",
            "    return await run_in_threadpool(dependant.call, **values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/concurrency.py\", line 37, in run_in_threadpool\n",
            "    return await anyio.to_thread.run_sync(func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-14-782769594.py\", line 44, in analyze_report\n",
            "    outputs = model.generate(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\", line 1968, in generate\n",
            "    outputs = self.base_model.generate(*args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 1738, in unsloth_fast_generate\n",
            "    output = self._old_generate(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2625, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3609, in _sample\n",
            "    outputs = model_forward(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/mistral.py\", line 232, in MistralForCausalLM_fast_forward\n",
            "    outputs = LlamaModel_fast_forward_inference(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 1056, in LlamaModel_fast_forward_inference_custom\n",
            "    X, present_key_value = attention_fast_forward_inference(\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 283, in LlamaAttention_fast_forward_inference\n",
            "    RH_Q = self.RH_Q\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1928, in __getattr__\n",
            "    raise AttributeError(\n",
            "AttributeError: 'MistralAttention' object has no attribute 'RH_Q'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     197.26.153.79:0 - \"OPTIONS /analyze HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     197.26.153.79:0 - \"POST /analyze HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 112, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 93, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 144, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 714, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 734, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 214, in run_endpoint_function\n",
            "    return await run_in_threadpool(dependant.call, **values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/concurrency.py\", line 37, in run_in_threadpool\n",
            "    return await anyio.to_thread.run_sync(func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-14-782769594.py\", line 44, in analyze_report\n",
            "    outputs = model.generate(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\", line 1968, in generate\n",
            "    outputs = self.base_model.generate(*args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 1738, in unsloth_fast_generate\n",
            "    output = self._old_generate(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2625, in generate\n",
            "    result = self._sample(\n",
            "             ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3609, in _sample\n",
            "    outputs = model_forward(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/mistral.py\", line 232, in MistralForCausalLM_fast_forward\n",
            "    outputs = LlamaModel_fast_forward_inference(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 1056, in LlamaModel_fast_forward_inference_custom\n",
            "    X, present_key_value = attention_fast_forward_inference(\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\", line 265, in LlamaAttention_fast_forward_inference\n",
            "    Qn = fast_linear_forward(self.q_proj, Xn, out = self.temp_QA[0])\n",
            "                                                    ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1928, in __getattr__\n",
            "    raise AttributeError(\n",
            "AttributeError: 'MistralAttention' object has no attribute 'temp_QA'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAW DECODED OUTPUT ===\n",
            "### Instruction:\n",
            "You are a medical assistant. Read the following medical report and provide:\n",
            "\n",
            "1. A short summary\n",
            "2. Your interpretation\n",
            "3. A solution â€” structured as multiple clearly numbered steps (e.g., 1., 2., 3.) on separate lines\n",
            "\n",
            "### Report:\n",
            "A 30-year-old female presents with high-grade fever, severe headache, photophobia, and neck stiffness. She is lethargic and has a positive Brudzinski sign.\n",
            "\n",
            "### Response:\n",
            "1. Summary: The patient has meningitis.\n",
            "2. Interpretation: Meningitis is an inflammation of the membranes that cover the brain and spinal cord. It can be caused by bacteria or viruses. Symptoms include fever, headache, nausea, vomiting, confusion, seizures, and stiff neck. Treatment includes antibiotics for bacterial infections and antiviral medications for viral infections.\n",
            "3. Solution: Administer IV fluids to maintain hydration; administer pain medication such as acetaminophen or ibuprofen; monitor vital signs closely; consult with neurologist if symptoms worsen or do not improve within 48 hours after starting treatment.\n",
            "INFO:     197.26.153.79:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     197.26.153.79:0 - \"OPTIONS /analyze HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     197.26.153.79:0 - \"OPTIONS /analyze HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAW DECODED OUTPUT ===\n",
            "### Instruction:\n",
            "You are a medical assistant. Read the following medical report and provide:\n",
            "\n",
            "1. A short summary\n",
            "2. Your interpretation\n",
            "3. A solution â€” structured as multiple clearly numbered steps (e.g., 1., 2., 3.) on separate lines\n",
            "\n",
            "### Report:\n",
            "A 55-year-old man presents with chest pain radiating to the left arm, shortness of breath, and diaphoresis. ECG shows ST-elevation in leads II, III, and aVF.\n",
            "\n",
            "### Response:\n",
            "1. Summary: The patient is experiencing an acute myocardial infarction.\n",
            "2. Interpretation: The patient needs immediate treatment for his condition.\n",
            "3. Solution: Administer    1. Call emergency services <br />/p>\n",
            "INFO:     197.26.153.79:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "=== RAW DECODED OUTPUT ===\n",
            "### Instruction:\n",
            "You are a medical assistant. Read the following medical report and provide:\n",
            "\n",
            "1. A short summary\n",
            "2. Your interpretation\n",
            "3. A solution â€” structured as multiple clearly numbered steps (e.g., 1., 2., 3.) on separate lines\n",
            "\n",
            "### Report:\n",
            "A 55-year-old man presents with chest pain radiating to the left arm, shortness of breath, and diaphoresis. ECG shows ST-elevation in leads II, III, and aVF.\n",
            "\n",
            "### Response:\n",
            "1. Summary: The\n",
            "The patient is experiencing anï¿½Â°2.esc>chest pain that is likely due to coronary artery disease. He should be given aspirin and nitroglycerine immediately.\n",
            "\n",
            "2. Interpretation:\n",
            "The patient has had a heart attack and needs immediate treatment.\n",
            "\n",
            "3. Solution:\n",
            "1. Administer aspirin and nitroglycerine.\n",
            "2. Call for help from a cardiologist or emergency room physician.\n",
            "3. Monitor vital signs and symptoms closely.\n",
            "INFO:     197.26.153.79:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     197.26.153.79:0 - \"OPTIONS /analyze HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RAW DECODED OUTPUT ===\n",
            "### Instruction:\n",
            "You are a medical assistant. Read the following medical report and provide:\n",
            "\n",
            "1. A short summary\n",
            "2. Your interpretation\n",
            "3. A solution â€” structured as multiple clearly numbered steps (e.g., 1., 2., 3.) on separate lines\n",
            "\n",
            "### Report:\n",
            "A 55-year-old man presents with chest pain radiating to the left arm, shortness of breath, and diaphoresis. ECG shows ST-elevation in leads II, III, and aVF\n",
            "\n",
            "### Response:\n",
            "1. Summary: The patient is experiencing an acute myocardial infarction. He should be given aspirin immediately and transported to the nearest hospital for emergency treatment.\n",
            "\n",
            "2. Interpretation: The patient has had a heart attack. This means that one or more of his coronary arteries have become blocked by blood clots, preventing oxygen from reaching part of his heart muscle. If this continues, it can cause permanent damage to the affected area(s) of the heart.\n",
            "\n",
            "3. Solution: Give the patient aspirin immediately and transport him to the nearest hospital for emergency treatment.\n",
            "INFO:     197.26.153.79:0 - \"POST /analyze HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "!./cloudflared tunnel --url http://localhost:8000 --no-autoupdate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://useful-candle-9d.trycloudflare.com/analyze\"  # Replace with your actual URL\n",
        "\n",
        "response = requests.post(url, json={\n",
        "    \"report\": \"A 70-year-old patient reports difficulty breathing and swelling in the ankles...\"\n",
        "})\n",
        "\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "tKeiIgfRoqBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f0c3c84d6ee54443a6425d6fe4d03ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19993a32685c42719a4a4c2150f45ef3",
              "IPY_MODEL_6a05c86e91be44f890bfd1014be9a226",
              "IPY_MODEL_e2c2240984eb4186a8b05f1a820e6810"
            ],
            "layout": "IPY_MODEL_d8699d4ba45e4019b8524009c52db26e"
          }
        },
        "19993a32685c42719a4a4c2150f45ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5454e0eb0edc454f9dc2275c9bad2898",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_870751378eef4858aa8be004243194f9",
            "value": "Map:â€‡100%"
          }
        },
        "6a05c86e91be44f890bfd1014be9a226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_194b8c6f5e2c44fd955a080aec623e05",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54614ebbc1424ff5b5251e09e4677b61",
            "value": 8
          }
        },
        "e2c2240984eb4186a8b05f1a820e6810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16de58dfc3fb4d8995b9503f11883bdf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cc7ade48e6af4e75956c427a68564694",
            "value": "â€‡8/8â€‡[00:00&lt;00:00,â€‡246.89â€‡examples/s]"
          }
        },
        "d8699d4ba45e4019b8524009c52db26e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5454e0eb0edc454f9dc2275c9bad2898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "870751378eef4858aa8be004243194f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "194b8c6f5e2c44fd955a080aec623e05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54614ebbc1424ff5b5251e09e4677b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16de58dfc3fb4d8995b9503f11883bdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7ade48e6af4e75956c427a68564694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}